{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varun-dubagunta/Projects/blob/main/NLP_pre_processing_prac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STOP WORD REMOVAL\n"
      ],
      "metadata": {
        "id": "pIGDIgSjLUGP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxfo6CYL5zG2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import nltk #REMINDER TO SELF: USE nltk.download() for specific datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords') #basic stopwords dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bos5S-626cGc",
        "outputId": "1cab1fdd-fd25-4ba7-d5cb-be413368518f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "tJC8hHXj80e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab') #punkt is a library containinig tokeinzer methods based on an unsupervised model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGTltuQIFHzd",
        "outputId": "121837f8-4407-4824-8aab-a5de4fc9d7b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "stop_words  = np.asarray(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "word_arr = np.asarray(word_tokenize(\"marks loves to eat pizza in the dominos cafeteria at night\")) #process tokenized array as np.array\n",
        "\n",
        "word_arr = np.asarray([word for word in word_arr if word not in stop_words]) #removes stop words\n",
        "\n",
        "print (word_arr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW_2O1mr7Nen",
        "outputId": "de3be41d-3e7f-49d1-e266-a40f319ae243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['marks' 'loves' 'eat' 'pizza' 'dominos' 'cafeteria' 'night']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ACCENTED WORD REMOVAL"
      ],
      "metadata": {
        "id": "katdPRDSLX2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "test_text = \"HÉŁŁÖ WÖRŁD ÎÑ ÀĆĆÉÑTŠ\"\n",
        "\n",
        "def strip_accents(text):\n",
        "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  return text\n",
        "\n",
        "print(strip_accents(test_text)) ##expect character loss as Ł is not recognised"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcK8tnVKLePm",
        "outputId": "f66588c4-efd8-42e5-e9dd-6ea6aa66ccf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HEO WORD IN ACCENTS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPECIAL CHARACTER REMOVAL"
      ],
      "metadata": {
        "id": "w66TxFa_PqYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "test_text = \"hello the weather2 is nice3213 today!1213\"\n",
        "def remove_special_characters(text, rem_num = False):\n",
        "  text = re.sub('[^a-zA-Z0-9/s]',' ', text) if not rem_num else re.sub('[^a-zA-Z/s]',' ', text)\n",
        "  return text\n",
        "\n",
        "print(remove_special_characters(test_text),'\\n' ,remove_special_characters(test_text, rem_num = True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIbQxTObMWTk",
        "outputId": "3d883a8c-e7cc-4de4-90aa-ea4cad9272f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello the weather2 is nice3213 today 1213 \n",
            " hello the weather  is nice     today     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEMMING"
      ],
      "metadata": {
        "id": "tMEC-mB0TUWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import porter\n",
        "\n",
        "test_text = \"thinking about my favorite type of everything bagel seasoning\"\n",
        "def stemmer(text):\n",
        "  text_arr = np.asarray([porter.PorterStemmer().stem(word) for word in text.split()])\n",
        "\n",
        "  return text_arr\n",
        "\n",
        "print(stemmer(test_text)) #words will return inaccurate stems: used to illustrate need for lemmatization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEWXe9MYRdht",
        "outputId": "00f32fe5-85d5-4f51-b359-6c0eb19365a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['think' 'about' 'my' 'favorit' 'type' 'of' 'everyth' 'bagel' 'season']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEMMATIZATION"
      ],
      "metadata": {
        "id": "1eQDmSucTdAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "test_text = \"rocks are not part of flora and fauna, they are not species or belong to any genus\"\n",
        "\n",
        "def lemmatize(text):\n",
        "  text_arr = np.asarray([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "  return text_arr\n",
        "\n",
        "print(lemmatize(test_text)) #accurately extracts roots of specific words given redundant inflection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n84b45kuTg2Q",
        "outputId": "3f152c8d-baa7-4992-ada9-98c95ec92ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['rock' 'are' 'not' 'part' 'of' 'flora' 'and' 'fauna,' 'they' 'are' 'not'\n",
            " 'specie' 'or' 'belong' 'to' 'any' 'genus']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMM0IBV4Sx0l9bHdiQ9K8qS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}